{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6e468",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# models\n",
    "clf_down = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "clf_up = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "X_train = train.drop(columns=[\"ret_fut\",\"y_down\",\"y_up\"])\n",
    "X_test  = test.drop(columns=[\"ret_fut\",\"y_down\",\"y_up\"])\n",
    "\n",
    "y_down_train = train[\"y_down\"]\n",
    "y_up_train   = train[\"y_up\"]\n",
    "\n",
    "# fit\n",
    "clf_down.fit(X_train, y_down_train)\n",
    "clf_up.fit(X_train, y_up_train)\n",
    "\n",
    "# proba\n",
    "test[\"p_down\"] = clf_down.predict_proba(X_test)[:, 1]\n",
    "test[\"p_up\"]   = clf_up.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# thresholds\n",
    "THR_DOWN = 0.55\n",
    "THR_UP   = 0.55\n",
    "\n",
    "# meta signal\n",
    "test[\"mode\"] = \"baseline\"\n",
    "test.loc[test[\"p_down\"] > THR_DOWN, \"mode\"] = \"risk_off\"\n",
    "test.loc[(test[\"mode\"] == \"baseline\") & (test[\"p_up\"] > THR_UP), \"mode\"] = \"risk_on\"\n",
    "\n",
    "# build daily weights from modes\n",
    "dates_test = sorted(test.index.get_level_values(0).unique())\n",
    "\n",
    "# weights templates\n",
    "w_baseline = w_low.loc[dates_test].copy()\n",
    "w_risk_on  = pd.DataFrame(1.0, index=dates_test, columns=SYMBOLS)  # equal weight\n",
    "w_risk_off = pd.DataFrame(0.0, index=dates_test, columns=SYMBOLS)  # cash\n",
    "\n",
    "# map mode per date using mean prob across assets\n",
    "mode_by_date = (\n",
    "    test.reset_index()\n",
    "        .groupby(\"timestamp\")[[\"p_down\",\"p_up\",\"mode\"]]\n",
    "        .agg({\"p_down\":\"mean\",\"p_up\":\"mean\",\"mode\":lambda x: x.iloc[0]})\n",
    ")\n",
    "\n",
    "W = pd.DataFrame(index=dates_test, columns=SYMBOLS, dtype=float)\n",
    "\n",
    "# assign\n",
    "for d in dates_test:\n",
    "    m = mode_by_date.loc[d, \"mode\"]\n",
    "    if m == \"risk_off\":\n",
    "        W.loc[d] = w_risk_off.loc[d]\n",
    "    elif m == \"risk_on\":\n",
    "        W.loc[d] = w_risk_on.loc[d]\n",
    "    else:\n",
    "        W.loc[d] = w_baseline.loc[d]\n",
    "\n",
    "# normalize\n",
    "W = W.fillna(0.0)\n",
    "W = W.clip(lower=0.0)\n",
    "W = W.div(W.sum(axis=1).replace(0, np.nan), axis=0).fillna(0.0)\n",
    "\n",
    "# backtest util\n",
    "def run_bt(weights, rets, cost_bps=10):\n",
    "    w = weights.shift(1).reindex(rets.index).fillna(0.0)\n",
    "    w = w.div(w.sum(axis=1).replace(0, np.nan), axis=0).fillna(0.0)\n",
    "    gross = (w * rets).sum(axis=1)\n",
    "\n",
    "    to = w.diff().abs().sum(axis=1).fillna(0.0)\n",
    "    cost = to * (cost_bps / 10000.0)\n",
    "\n",
    "    net = gross - cost\n",
    "    return net, to\n",
    "\n",
    "def stats(r, freq=365):\n",
    "    r = r.dropna()\n",
    "    eq = (1 + r).cumprod()\n",
    "    mu = r.mean() * freq\n",
    "    sig = r.std() * np.sqrt(freq)\n",
    "    sharpe = mu / sig if sig > 0 else np.nan\n",
    "    dd = (eq / eq.cummax() - 1.0).min()\n",
    "    ann = eq.iloc[-1] ** (freq/len(r)) - 1 if len(r) > 0 else np.nan\n",
    "    return float(ann), float(sig), float(sharpe), float(dd)\n",
    "\n",
    "# compare on test window only\n",
    "rets_test = rets.loc[dates_test]\n",
    "\n",
    "r_meta, to_meta = run_bt(W, rets_test, cost_bps=COST_BPS)\n",
    "r_low,  to_low  = run_bt(w_baseline, rets_test, cost_bps=COST_BPS)\n",
    "\n",
    "# print\n",
    "rows = []\n",
    "for name, r, to in [\n",
    "    (\"baseline_low_vol\", r_low, to_low),\n",
    "    (\"meta_two_models\",  r_meta, to_meta),\n",
    "]:\n",
    "    ann, sig, sh, dd = stats(r, freq=FREQ)\n",
    "    rows.append({\"name\": name, \"ann_return\": ann, \"ann_vol\": sig, \"sharpe\": sh, \"max_dd\": dd, \"turnover\": float(to.mean())})\n",
    "\n",
    "summary = pd.DataFrame(rows).set_index(\"name\")\n",
    "print(summary)\n",
    "\n",
    "# equity\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot((1+r_low.fillna(0)).cumprod(), label=\"baseline_low_vol\")\n",
    "plt.plot((1+r_meta.fillna(0)).cumprod(), label=\"meta_two_models\")\n",
    "plt.legend()\n",
    "plt.title(\"Test window equity\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# mode counts\n",
    "print(\"\\nmode counts\")\n",
    "print(mode_by_date[\"mode\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc134fea",
   "metadata": {},
   "source": [
    "                  | ann_return  | ann_vol | sharpe | max_dd  turnover |\n",
    "baseline_low_vol |-0.272841  | 0.510340 | -0.366125 | -0.449281 | 0.077413 |\n",
    "\n",
    "meta_two_models  | -0.288073 | 0.512886 |-0.403006 | -0.464904 |  0.141166 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140959fe",
   "metadata": {},
   "source": [
    "![Image test 8](image/image_test_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a470a",
   "metadata": {},
   "source": [
    "mode counts\n",
    "\n",
    "mode\n",
    "\n",
    "baseline    353\n",
    "\n",
    "risk_on      13\n",
    "\n",
    "Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c43fac",
   "metadata": {},
   "source": [
    "Sur la période test, la stratégie baseline low-vol est déjà en difficulté (rendement annualisé négatif), ce qui montre que le marché crypto était globalement défavorable sur cette fenêtre. La stratégie méta (2 modèles) ne fait pas mieux : elle obtient une performance légèrement pire et un drawdown un peu plus élevé, avec un turnover plus important.\n",
    "\n",
    "Un point clé : le modèle choisit presque toujours le mode baseline (353 jours), très rarement risk_on (13 jours) et jamais risk_off. Donc en pratique, la stratégie “switch” très peu, et quand elle switch, ça rajoute surtout du turnover (donc des coûts) sans apporter de protection suffisante.\n",
    "\n",
    "Ces résultats restent intéressants pour le projet : ils montrent que la pipeline ML (features → labels → train/test → signal → allocation) est bien construite, mais que la prédiction directionnelle des “gros moves” est difficile à ce stade. La suite logique serait de tester d’autres seuils, d’autres définitions de queues (ex: 10% au lieu de 30%), ou des modèles plus flexibles (arbres / boosting), tout en contrôlant le turnover."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
