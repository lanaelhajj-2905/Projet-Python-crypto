{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1aa7fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# labels thresholds computed on train only\n",
    "down_thr = train_full[\"ret_fut\"].quantile(Q_MOVE)\n",
    "up_thr   = train_full[\"ret_fut\"].quantile(1 - Q_MOVE)\n",
    "\n",
    "def make_labels(df):\n",
    "    y_down = (df[\"ret_fut\"] <= down_thr).astype(int)\n",
    "    y_up   = (df[\"ret_fut\"] >= up_thr).astype(int)\n",
    "    return y_down, y_up\n",
    "\n",
    "y_down_train, y_up_train = make_labels(train_full)\n",
    "y_down_test, y_up_test   = make_labels(test_final)\n",
    "\n",
    "X_train = train_full.drop(columns=[\"ret_fut\"])\n",
    "X_test  = test_final.drop(columns=[\"ret_fut\"])\n",
    "\n",
    "# models list\n",
    "models = {\n",
    "    \"logit\": Pipeline([(\"scaler\", StandardScaler()),\n",
    "                       (\"clf\", LogisticRegression(max_iter=3000, class_weight=\"balanced\"))]),\n",
    "    \"rf\": RandomForestClassifier(n_estimators=400, max_depth=6, random_state=42),\n",
    "    \"svm_rbf\": Pipeline([(\"scaler\", StandardScaler()),\n",
    "                         (\"clf\", SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\"))]),\n",
    "}\n",
    "\n",
    "# cv eval\n",
    "def eval_model(model, X, y, folds):\n",
    "    aps, aucs = [], []\n",
    "    for tr_dates, va_dates in folds:\n",
    "        tr_idx = X.index.get_level_values(0).isin(tr_dates)\n",
    "        va_idx = X.index.get_level_values(0).isin(va_dates)\n",
    "\n",
    "        Xtr, ytr = X.loc[tr_idx], y.loc[tr_idx]\n",
    "        Xva, yva = X.loc[va_idx], y.loc[va_idx]\n",
    "\n",
    "        if len(np.unique(ytr)) < 2 or len(np.unique(yva)) < 2:\n",
    "            continue\n",
    "\n",
    "        m = model\n",
    "        m.fit(Xtr, ytr)\n",
    "        p = m.predict_proba(Xva)[:, 1]\n",
    "        aps.append(average_precision_score(yva, p))\n",
    "        aucs.append(roc_auc_score(yva, p))\n",
    "    return float(np.mean(aps)) if aps else np.nan, float(np.mean(aucs)) if aucs else np.nan\n",
    "\n",
    "# benchmark basic\n",
    "rows = []\n",
    "for name, model in models.items():\n",
    "    ap_d, auc_d = eval_model(model, X_train, y_down_train, folds)\n",
    "    ap_u, auc_u = eval_model(model, X_train, y_up_train, folds)\n",
    "    rows.append({\"model\": name, \"AP_down\": ap_d, \"AUC_down\": auc_d, \"AP_up\": ap_u, \"AUC_up\": auc_u})\n",
    "\n",
    "bench = pd.DataFrame(rows).sort_values(\"AP_down\", ascending=False)\n",
    "print(\"=== CV benchmark (train only) ===\")\n",
    "print(bench)\n",
    "\n",
    "# pick best model family per task\n",
    "best_down_name = bench.sort_values(\"AP_down\", ascending=False).iloc[0][\"model\"]\n",
    "best_up_name   = bench.sort_values(\"AP_up\", ascending=False).iloc[0][\"model\"]\n",
    "\n",
    "print(\"\\nbest_down:\", best_down_name, \"best_up:\", best_up_name)\n",
    "\n",
    "# gridsearch light per best family\n",
    "grids = {\n",
    "    \"logit\": [\n",
    "        {\"clf__C\": [0.1, 0.5, 1.0, 2.0]},\n",
    "    ],\n",
    "    \"rf\": [\n",
    "        {\"n_estimators\": [300, 600],\n",
    "         \"max_depth\": [3, 6, 10],\n",
    "         \"min_samples_leaf\": [1, 5, 10]},\n",
    "    ],\n",
    "    \"svm_rbf\": [\n",
    "        {\"clf__C\": [0.5, 1.0, 2.0],\n",
    "         \"clf__gamma\": [\"scale\", 0.1, 0.01]},\n",
    "    ],\n",
    "}\n",
    "\n",
    "def clone_model(name):\n",
    "    return models[name]\n",
    "\n",
    "def grid_search(name, y):\n",
    "    best = {\"name\": name, \"params\": None, \"AP\": -np.inf, \"AUC\": -np.inf}\n",
    "    for params in ParameterGrid(grids[name]):\n",
    "        m = clone_model(name)\n",
    "        m.set_params(**params)\n",
    "        ap, auc = eval_model(m, X_train, y, folds)\n",
    "        if np.isnan(ap):\n",
    "            continue\n",
    "        if ap > best[\"AP\"]:\n",
    "            best = {\"name\": name, \"params\": params, \"AP\": ap, \"AUC\": auc}\n",
    "    return best\n",
    "\n",
    "best_down = grid_search(best_down_name, y_down_train)\n",
    "best_up   = grid_search(best_up_name,   y_up_train)\n",
    "\n",
    "print(\"\\n=== best after grid (train CV) ===\")\n",
    "print(\"DOWN:\", best_down)\n",
    "print(\"UP  :\", best_up)\n",
    "\n",
    "# fit final on full train, test on final year\n",
    "def fit_and_test(best, y_train, y_test):\n",
    "    m = clone_model(best[\"name\"])\n",
    "    if best[\"params\"] is not None:\n",
    "        m.set_params(**best[\"params\"])\n",
    "    m.fit(X_train, y_train)\n",
    "    p = m.predict_proba(X_test)[:, 1]\n",
    "    ap = average_precision_score(y_test, p)\n",
    "    auc = roc_auc_score(y_test, p)\n",
    "    return m, p, float(ap), float(auc)\n",
    "\n",
    "m_down, p_down, ap_down, auc_down = fit_and_test(best_down, y_down_train, y_down_test)\n",
    "m_up,   p_up,   ap_up,   auc_up   = fit_and_test(best_up,   y_up_train,   y_up_test)\n",
    "\n",
    "print(\"\\n=== FINAL TEST (last 1y) ===\")\n",
    "print(\"DOWN AP:\", ap_down, \"AUC:\", auc_down)\n",
    "print(\"UP   AP:\", ap_up,   \"AUC:\", auc_up)\n",
    "\n",
    "# signal coverage diagnostics\n",
    "# trigger high confidence quantile based on train probs\n",
    "train_pdown = m_down.predict_proba(X_train)[:, 1]\n",
    "train_pup   = m_up.predict_proba(X_train)[:, 1]\n",
    "\n",
    "thr_down = float(np.quantile(train_pdown, 0.95))\n",
    "thr_up   = float(np.quantile(train_pup,   0.95))\n",
    "\n",
    "cov_down = float((p_down >= thr_down).mean())\n",
    "cov_up   = float((p_up >= thr_up).mean())\n",
    "\n",
    "print(\"thr_down:\", thr_down, \"coverage_down:\", cov_down)\n",
    "print(\"thr_up  :\", thr_up,   \"coverage_up  :\", cov_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3719b5",
   "metadata": {},
   "source": [
    "=== CV benchmark (train only) ===\n",
    "\n",
    "     model   AP_down  AUC_down     AP_up    AUC_up\n",
    "0    logit  0.346724  0.596781  0.345489  0.546129\n",
    "1       rf  0.332014  0.585816  0.339219  0.558626\n",
    "2  svm_rbf  0.329953  0.578053  0.331228  0.550104\n",
    "\n",
    "best_down: logit best_up: logit\n",
    "\n",
    "=== best after grid (train CV) ===\n",
    "\n",
    "DOWN: {'name': 'logit', 'params': {'clf__C': 2.0}, 'AP': 0.34674515004628387, 'AUC': 0.5967473689866093}\n",
    "UP  : {'name': 'logit', 'params': {'clf__C': 0.1}, 'AP': 0.34564588138708224, 'AUC': 0.5461791044376753}\n",
    "\n",
    "=== FINAL TEST (last 1y) ===\n",
    "\n",
    "DOWN AP: 0.37792683223062695 AUC: 0.5795918051363537\n",
    "UP   AP: 0.33955081724059694 AUC: 0.568304707379135\n",
    "thr_down: 0.6346630567397514 coverage_down: 0.02185792349726776\n",
    "thr_up  : 0.6346630567397514 coverage_up  : 0.04052823315118397"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82218c",
   "metadata": {},
   "source": [
    "Le benchmark CV montre que la régression logistique reste le meilleur choix parmi les trois familles testées, même face à des modèles plus complexes (RF, SVM). Sur la dernière année, les scores restent modestes (AUC autour de 0.57–0.58), ce qui confirme que prédire la direction des “gros moves” est difficile avec ces features simples.\n",
    "\n",
    "Un point important est la couverture très faible quand on déclenche uniquement les signaux à très haute confiance (quantile 95% des probas) : environ 2% des observations pour le signal “down” et 4% pour le signal “up”. Donc le modèle peut être exploitable comme filtre “rare mais fort”, mais en pratique il activera très peu souvent, ce qui limite l’impact sur une stratégie d’allocation si on garde des seuils aussi stricts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
